{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "\n",
    "## Hidden Markov models\n",
    "\n",
    "\n",
    "Consider the evolution of the state $\\{x_k\\}$ and the partial observation $\\{y_k\\}$ at time $k$\n",
    "$$ \n",
    "\\begin{align*}\n",
    "x_{k+1} = f(x_k) + \\omega_{k+1}\\\\\n",
    "y_{k+1} = h(x_k) + \\nu_{k+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "where evolution error $\\omega_{k+1}$ and observation error $\\nu_{k+1}$ are drawn from Gaussian distributons of known covarainces $\\Sigma_{\\omega}$ and $\\Sigma_{\\nu}$.\n",
    "\n",
    "\n",
    "\n",
    "## Filtering algorithm\n",
    "\n",
    "Different filtering algorithms are designed to estimate the distribution of the state $\\{x_k\\}$\n",
    "$$p(x_k|Y_{k}) \\qquad \\textrm{where} \\qquad Y_k = \\{y_1, y_2,\\cdots, y_k\\}$$\n",
    "\n",
    "The prediction step involves the Chapman-Kolmogorov equation \n",
    "$$ \n",
    "p(x_{k+1}|Y_{k}) = \\int p(x_{k+1}|x_k) p(x_{k}|Y_{k}) dx_{k} = \\int \\frac{1}{\\sqrt{(2\\pi)^{N_{x}}|\\Sigma_{\\omega}|}} e^{-\\frac{1}{2}(x_{k+1} - f(x_{k}))^T\\Sigma_{\\omega}^{-1}(x_{k+1} - f(x_{k}))} p(x_{k}|Y_{k}) dx_{k} \\\\\n",
    "$$\n",
    "The analysis step involves Bayes' rule\n",
    "$$ \n",
    "p(x_{k+1}|Y_{k+1}) = p(x_{k+1}|y_{k+1}, Y_{k}) = \\frac{p(y_{k+1}|x_{k+1})p(x_{k+1}|Y_{k})}{p(y_{k+1}|Y_k)} \\propto e^{-\\frac{1}{2}(y_{k+1} - h(x_{k+1}))^T\\Sigma_{\\nu}^{-1}(y_{k+1} - h(x_{k+1}))}p(x_{k+1}|Y_{k})\n",
    "$$\n",
    "\n",
    "Combining prediction and analysis steps leads to the recursive relation $p(x_{k}|Y_{k}) \\rightarrow p(x_{k+1}|Y_{k+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filtering \n",
    "\n",
    "## Sequential Importance Sampling\n",
    "Consider the full posterior probability density $p(x_{0:k}| Y_k)$ given the measurements $Y_k$, we have the following recursion\n",
    "$$p(x_{0:k}|Y_k) \\propto p(y_k|x_k) p(x_k|x_{k-1})p(x_{0:{k-1}}|Y_{k-1}) $$\n",
    "\n",
    "To sample $x_{0:k}$, we could first construct an importance distribution $x_{0:k}^{j} \\sim \\pi(x_{0:k}|Y_k)$ and compute the corresponding imporance weights as \n",
    "$$\\omega_k^{j} \\propto \\frac{p(y_k|x_k^j) p(x_k^j|x_{k-1}^j)p(x_{0:{k-1}}^j|Y_{k-1})}{\\pi(x_{0:k}^j|Y_k)} $$\n",
    "\n",
    "To sequentially sample $x_{0:k}$, let's form the importance distribution recursively as follows\n",
    "$$\\pi(x_{0:k}|Y_k) = \\pi(x_k|x_{0:k-1},Y_k) \\pi(x_{0:k-1}|Y_{k-1})$$\n",
    "\n",
    "In the prediction step, we draw $J$ new samples $x_k^j$ from importance distributions\n",
    "$$x_k^j \\sim \\pi(x_k|x_{0:k-1},Y_k)$$\n",
    "In the analysis step, we update the weights\n",
    "$$\\omega_k^{j} \\propto \\frac{p(y_k|x_k^j) p(x_k^j|x_{k-1}^j)p(x_{0:{k-1}}^j|Y_{k-1})}{\\pi(x_{0:k}^j|Y_k)} = \\frac{p(y_k|x_k^j) p(x_k^j|x_{k-1}^j)}{\\pi(x_{k}^j|x_{0:k-1}^j, Y_k)}\\omega_{k-1}^j$$\n",
    "\n",
    "Resampling is required to to avoid converging to single non-zero weight $\\omega^i = 1$, which can be monitored by the effective number of samples \n",
    "$$N_{eff} = \\frac{1}{\\sum_{j=1}^J(\\omega_k^{j})^2}$$\n",
    "\n",
    "### Bootstrap Filter\n",
    "In bootstrap filter, we use dynamic model as the importance distribution\n",
    "$$\\pi(x_{k}^j|x_{0:k-1}^j, Y_k) \\propto p(x_{k}^j|x_{k-1}^j) \\omega_{k-1}^j$$\n",
    "\n",
    "We will draw points $\\{x_{k}^j\\}$ from the dynamic model:\n",
    "$$x_{k}^j \\sim p(x_k | x_{k-1}^{j})$$\n",
    "And calculate new weights \n",
    "$$\\omega_k^{j} \\propto p(y_k | x_{k}^{j})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filtering\n",
    "\n",
    "Kalman filtering can be viewed as the following recursive relationship\n",
    "$$\n",
    "\\begin{align*}\n",
    "&p(x_k|Y_{k}) = \\mathcal{N}(m_k, C_k)\\\\\n",
    "&p(x_{k+1}|Y_{k}) = \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1})\\\\\n",
    "&p(x_{k+1}|Y_{k+1}) = \\mathcal{N}(m_{k+1}, C_{k+1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The prediction step estimates\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{m}_{k+1} = \\mathbb{E}[x_{k+1} | Y_{k+1}] &= \\int\\int x_{k+1} p(x_{k+1}|x_k) p(x_k|Y_k) d x_k  dx_{k+1} \\\\\n",
    "                                             &= \\int f(x_{k}) p(x_k|Y_k) d x_k \\\\\n",
    "                                             & = \\mathbb{E}[f(x_{k}) | Y_{k}] \\\\\n",
    "\\hat{C}_{k+1} = \\textrm{Cov}[x_{k+1} | Y_{k+1}] &= \\int\\int (x_{k+1} - \\hat{m}_{k+1})(x_{k+1} - \\hat{m}_{k+1})^T p(x_{k+1}|x_k) p(x_k|Y_k) d x_k  dx_{k+1} \\\\\n",
    "                                             &= \\int \\Bigl[f(x_{k}) f(x_{k})^T + \\Sigma_{\\nu} \\Bigr] p(x_k|Y_k) d x_k  - \\hat{m}_{k+1} \\hat{m}_{k+1}^T\\\\\n",
    "                                             & = \\mathrm{Cov}[f(x_{k}) | Y_{k}] + \\Sigma_{\\nu}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The analysis step first apprximates the joint distribution of $p(x_{k+1}, y_{k+1} | Y_{k})$ as a Gaussian distribution\n",
    "$$\\begin{equation}\n",
    "     \\mathcal{N}\\Bigl(\n",
    "    \\begin{bmatrix}\n",
    "    \\hat{m}_{k+1}\\\\\n",
    "    \\hat{y}_{k+1}\n",
    "    \\end{bmatrix}, \n",
    "    \\begin{bmatrix}\n",
    "   \\hat{C}_{k+1} & \\hat{C}_{k+1}^{x y}\\\\\n",
    "    {\\hat{C}_{k+1}^{x y}}{}^{T} & \\hat{C}_{k+1}^{yy}\n",
    "    \\end{bmatrix}\n",
    "    \\Bigr).\n",
    "\\end{equation}$$\n",
    "\n",
    "Then, with $\\mathbb{E}$ denoting expectation with respect to \n",
    "$x_{k+1}|Y_k \\sim \\mathcal{N}( \\hat{m}_{k+1},\\hat{C}_{k+1})$, \n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\hat{y}_{k+1} =     & \\mathbb{E}[h(x_{k+1})|Y_k], \\\\\n",
    "    \\hat{C}_{k+1}^{x y} =     &  \\mathrm{Cov}[x_{k+1}, h(x_{k+1})|Y_k],\\\\\n",
    "    \\hat{C}_{k+1}^{y y} = &  \\mathrm{Cov}[h(x_{k+1})|Y_k] + \\Sigma_{\\nu}.\n",
    "\\end{align*}$$\n",
    "\n",
    "Conditioning the Gaussian to find $x_{k+1}|\\{Y_k,y_{k+1}\\}=x_{k+1}|Y_{k+1}$ gives the following\n",
    "expressions for the mean $m_{k+1}$ and covariance $C_{k+1}$ :\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        m_{k+1} &= \\hat{m}_{k+1} + \\hat{C}_{k+1}^{x y} (\\hat{C}_{k+1}^{y y})^{-1} (y_{k+1} - \\hat{y}_{k+1}),\\\\\n",
    "        C_{k+1} &= \\hat{C}_{k+1} - \\hat{C}_{k+1}^{x y}(\\hat{C}_{k+1}^{y y})^{-1} {\\hat{C}_{k+1}^{x y}}{}^{T}.\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Different Kalman filters are designed for Gaussian integrations \n",
    "$$\\mathbb{E}[f(x_{k}) | Y_{k}], \\, \\mathrm{Cov}[f(x_{k}) | Y_{k}],\\, \\mathbb{E}[h(x_{k+1})|Y_k],\\,\\mathrm{Cov}[x_{k+1}, h(x_{k+1})|Y_k],\\,\\textrm{and}\\, \\mathrm{Cov}[h(x_{k+1})|Y_k]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. [A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=978374)\n",
    "2. [A Tutorial on Particle Filtering and Smoothing: Fifteen years later](https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/johansen/publications/dj11.pdf)\n",
    "3. [Nonlinear Bayesian Estimation: From Kalman Filtering to a Broader Horizon](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283968)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
