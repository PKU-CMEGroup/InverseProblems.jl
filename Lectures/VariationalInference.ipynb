{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "Variational inference is an algorithm to sample the posterior distribution with an unknown normalization factor $Z(y)$\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\rho_{\\rm post}(\\theta | y) = \\frac{\\rho(\\theta, y)}{Z(y)} =  \\frac{\\rho_{\\rm prior}(\\theta) \\rho(y | \\theta)}{Z(y)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We consider the case that the conditional probability $\\rho(y | \\theta)$ and the prior $\\rho_{\\rm prior}(\\theta)$ are easy to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic variational inference algorithm\n",
    "\n",
    "The basic idea of variational inference is to find a simpler distribution $q_{\\lambda}(\\theta)$, which is parameterized by $\\lambda$, to approximate the original\n",
    "distribution $\\rho_{\\rm post}(\\theta | y)$\n",
    "\n",
    "$KL$ divergence is widely used to measure the distance between these distributions, \n",
    "$$\n",
    "\\begin{align*}\n",
    "KL\\Bigl[q_{\\lambda}(\\theta) \\Vert  \\rho_{\\rm post}(\\theta | y)\\Bigr] &= \\int q_{\\lambda}(\\theta)  \\log \\frac{q_{\\lambda}(\\theta)}{\\rho_{\\rm post}(\\theta | y)} d\\theta \\\\\n",
    "&= \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)}  \\Bigl[ \\log \\frac{q_{\\lambda}(\\theta)}{\\rho_{\\rm post}(\\theta | y)} \\Bigr]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The goal is obtain an optimal $\\lambda$, which minimizes the $KL$ divergence.  A natural idea is to use gradient descent method\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\lambda} KL\\Bigl[q_{\\lambda}(\\theta) \\Vert  \\rho_{\\rm post}(\\theta | y)\\Bigr] &= \\nabla_{\\lambda} \\int q_{\\lambda}(\\theta)  \\log \\frac{q_{\\lambda}(\\theta)}{\\rho_{\\rm post}(\\theta | y)} d\\theta \\\\\n",
    "&=  \\int \\nabla_{\\lambda} q_{\\lambda}(\\theta)  \\Bigl( \\log q_{\\lambda}(\\theta) - \\log \\rho_{\\rm prior}(\\theta) - \\log \\rho (y|\\theta)\\Bigr) \\\\\n",
    "&=  \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)} \\Bigl[ \\nabla_{\\lambda} \\log q_{\\lambda}(\\theta)  \\Bigl( \\log q_{\\lambda}(\\theta) - \\log \\rho_{\\rm prior}(\\theta) - \\log \\rho (y|\\theta)\\Bigr) \\Bigr]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here we use the fact \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\int \\nabla_{\\lambda} q_{\\lambda}(\\theta) d\\theta  = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "It is worth noticing that the gradient does not depend on the unknown normalization factor $Z(y)$. And the expectation can be approximated by Monte Carlo methods.\n",
    "\n",
    "\n",
    "### Evidence lower bound\n",
    "$KL$ divergence can be written as \n",
    "$$\n",
    "\\begin{align*}\n",
    "KL\\Bigl[q_{\\lambda}(\\theta) \\Vert  \\rho_{\\rm post}(\\theta | y)\\Bigr] &= \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)}  \\Bigl[ \\log \\frac{q_{\\lambda}(\\theta)}{\\rho_{\\rm post}(\\theta | y)} \\Bigr] \\\\\n",
    "&= Z_y - \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)}  \\Bigl[ \\log \\frac{\\rho(\\theta, y)}{q_{\\lambda}(\\theta)} \\Bigr] \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The evidence lower bound $ELBO(\\lambda)$ is defined as \n",
    "$$\n",
    "\\begin{align*}\n",
    "ELBO(\\lambda) = \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)}  \\Bigl[ \\log \\frac{\\rho(\\theta, y)}{q_{\\lambda}(\\theta)} \\Bigr] \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, minimizing $KL$ divergence is equivalent to maximizing $ELBO(\\lambda)$. And they have the same gradient (with different signs) with respect to $\\lambda$.\n",
    "\n",
    "And we have \n",
    "$$\n",
    "\\begin{align*}\n",
    "log \\rho(y) \\geq ELBO(\\lambda) = \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)}  \\Bigl[ \\log \\frac{\\rho(\\theta, y)}{q_{\\lambda}(\\theta)} \\Bigr] \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean field approximation\n",
    "\n",
    "We assume that the parameterized distribution has some nice structures as following\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_{\\lambda}(\\theta) = \\Pi_{i=1}^{m}q_{\\lambda_i}(\\theta_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$KL$ divergence becomes\n",
    "$$\n",
    "\\begin{align*}\n",
    "KL\\Bigl[q_{\\lambda}(\\theta) \\Vert  \\rho_{\\rm post}(\\theta | y)\\Bigr] \n",
    "&= \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)}  \\Bigl[ \\sum_{i=1}^{m} \\log q_{\\lambda_i}(\\theta_i) - \\log \\rho_{\\rm post}(\\theta | y) \\Bigr]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This can be minimized with the coordinate descent method, namely sequentially minimize each $\\lambda_i$. The $KL$ divergence can be rewritten as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "KL\\Bigl[q_{\\lambda}(\\theta) \\Vert  \\rho_{\\rm post}(\\theta | y)\\Bigr] \n",
    "&= \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)}  \\Bigl[ \\sum_{i=1}^{m} \\log q_{\\lambda_i}(\\theta_i) - \\log \\rho_{\\rm post}(\\theta | y) \\Bigr]\\\\\n",
    "&= \\sum_{i=1}^{m} \\mathbb{E}_{\\theta_i \\sim q_{\\lambda_i}(\\theta_i)}\\log q_{\\lambda_i}(\\theta_i) - \\mathbb{E}_{\\theta \\sim q_{\\lambda}(\\theta)} \\log \\Bigl(\\rho_{\\rm post}(\\theta_{-i_0} | y)\\rho_{\\rm post}(\\theta_{i_0} | \\theta_{-i_0} , y) \\Bigr) \\\\\n",
    "&=  \\mathbb{E}_{\\theta_{i_0} \\sim q_{\\lambda_{i_0}}(\\theta_{i_0})}\\Bigl[ \\log q_{\\lambda_{i_0}}(\\theta_{i_0}) - \n",
    "\\mathbb{E}_{\\theta_{-i_0} \\sim q_{\\lambda_{-i_0}}(\\theta_{-i_0})} \\log \\rho_{\\rm post}(\\theta_{i_0} | \\theta_{-i_0} , y)\n",
    "\\Bigr] + C\n",
    "\\end{align*}\n",
    "$$\n",
    "here other terms in $C$ are independent of $\\lambda_{i_0}$. Let denote \n",
    "$$\n",
    "h_{i_0}(\\cdot) = \\exp\\Bigl( \\mathbb{E}_{\\theta_{-i_0} \\sim q_{\\lambda_{-i_0}}(\\theta_{-i_0})} \\log \\rho_{\\rm post}(\\cdot | \\theta_{-i_0} , y) \\Bigr)\n",
    "$$\n",
    "\n",
    "The optimal solution satisfies\n",
    "$$\n",
    "q_{\\lambda_{i_0}}(\\cdot) \\propto h_{i_0}(\\cdot)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stein variational gradient descent\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. [Lecture 5: Variational Inference (Stanford Canvas)](https://canvas.stanford.edu/files/1780120/download?download_frd=1&verifier=MWyibVq7L4EmRgunWLV7pS7CekAI9MLuTJIHxuCV;Lecture+5.pdf;application/pdf)\n",
    "2. [Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm](https://proceedings.neurips.cc/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
